To be filled:
1) Kraken2
2) Nonpareil

TrimGalore

trim_galore --length 75 --cores 24 --paired <<the 2 pairs>>
check under pgrep -a perl
expect 2 .fq with val on the name

metaspades

https://github.com/ablab/spades#sec3.2

1) Install via conda
2) Command
  a) --meta, -t, -m, -k 27,37,47,57,67,77,87,97,107,117,127 

megahit

https://github.com/voutcn/megahit

1) Install via conda environment
2) Command (27 to 127 seems preferable)
  - megahit -1 pe_1.fq -2 pe_2.fq -o out --k-min 27 --k-max 127 --k-step 10 --min-contig-len 1000 -t 96 -o <<>>
metaCarvel

https://github.com/marbl/MetaCarvel/wiki/5.-Interpreting-the-output

1) Install as per github page
  - conda activate py2 (install everything under python 2 environment)
  - Check samtools, bedtools, numpy, bowtie2 through conda
  - pip install networkx==1.10
2) Prepare data using bowtie2 and samtools as shown
    - bowtie2-build --threads 96 contigs.fasta idx
    - bowtie2 --threads 96 -x idx -U first.fastq.gz | samtools view -bS -@ 96 - | samtools sort -@ 96 - -o alignment_1.bam (do the same for the R2 lib)
    - samtools merge -@ 96 alignment_total.bam alignment_1.bam alignment_2.bam
    - samtools sort -@ 96 -n alignment_total.bam -o alignment.bam
3) Command
  - python <<*Carvel/run.py>> -a <<*83a/*127/*.fa>> -m <<*83a/*bowtie/alignment.bam>> -d <<MBS1383mega127filcarvrk>> -r true -l 1000
  - Note: always keep -r and -k true. -l 1000 for contigs above 1000 only for binning 

minCED 

https://github.com/ctSkennerton/minced

1) Install minCED through conda
2) Command
  - minced -minNR 2 (or 3) -minRL 16 -maxRL 64 -minSL 8 -maxSL 72 <<.fa>> <<.txt>>

cctyper

https://github.com/Russel88/CRISPRCasTyper/blob/master/bin/cctyper

1) Install cctyper as above link specified, activate cctyper environment
2) For chromosomes: cctyper <<input fasta>> <<output file directory name>> --prodigal single --circular -t <<thread number>> --keep_tmp 
For scaffold/contig: cctyper <<input fasta>> <<output file directory name>> --prodigal meta -t <<thread number>> --keep_tmp
For meta: similar to scaffold/contig


For binning:

1) Preparation (alignment)

bowtie2-build --threads  scaffold.fasta idx
bowtie2 --threads 96 -x idx -1 first.fastq.gz -2 first.fastq.gz -S *.sam

  (the ff 2 commands are for maxbin2)
  pileup.sh in=megahit.sam  out=cov.txt
  awk '{print $1"\t"$5}' cov.txt | grep -v '^#' > abundance.txt
  
samtools view -b -S -@ 96 -o *.bam *.sam (or if header missing samtools view -bT MBS1383metacarvel.fa MBS1383aln.sam > MBS1383aln.bam)
samtools sort -@ 96 MBS1384aln.bam -o MBS1384alnsorted (samtools flagstat bwaOutput.bam - to calculate for % reads aligned)
samtools index *sorted.bam
samtools flagstat sortedoutput.bam (report the alignment)

jgi_summarize_bam_contig_depths --outputDepth depth.txt *.bam

2) Preparation and running concoct

cut_up_fasta.py original_contigs.fa -c 10000 -o 0 --merge_last -b contigs_10K.bed > contigs_10K.fa
concoct_coverage_table.py contigs_10K.bed mapping/Sample*.sorted.bam > coverage_table.tsv (make sure sorted.bam.bai is available in the same folder at this point)
concoct --composition_file contigs_10K.fa --coverage_file coverage_table.tsv -t 96 -b concoct_output/
merge_cutup_clustering.py concoct_output/clustering_gt1000.csv > concoct_output/clustering_merged.csv
mkdir concoct_output/fasta_bins
extract_fasta_bins.py original_contigs.fa concoct_output/clustering_merged.csv --output_path concoct_output/fasta_bins (make sure you have mkdir)

3) Running the bins (metabat2 and maxbin2)

metabat2 -i MBS1383_metacarvel.fa -a MBS1383_metabat2_depth.txt -o MBS1383_metabat
run_MaxBin.pl -contig scaffold.fa -abund abundance.txt -out myout


Bins Merging

metawrap bin_refinement -o MBS1384_binmerged -t 64 -A *initialbins/metabat2_bins/ -B *initialbins/maxbin2_bins/ -C *initialbins/concoct_bins/ -c 50 -x 10

column -t metawrap_50_10_bins.stats | awk '{print $1, $2, $3, $2-(5*$3)}' | awk '$2>50 && $3<10 && $4>50' | wc -l (to check how many bins are with good quality)


RefineM

1) Refine based on composition

refinem scaffold_stats -c 96 --genome_ext .fa MBS1384metacarvel.fa *merged/metawrap_50_5_bins MBS1384refinem MBS1384alnsorted.bam (make sure that the *.bam and *.bam.bai is in the same directory)
      - output scaffold_stats.tsv and 3 more files
refinem outliers <stats_output_dir>/scaffold_stats.tsv <outlier_output_dir>
      - output outliers.tsv
refinem filter_bins --genome_ext .fa *merged/*5_bins *refinem/outliers.tsv *product1

2) Refine based on taxonomy

refinem call_genes --genome_ext .fa -c 96 *_1 MBS1384refinem_2 (use the bins refined based on composition)
refinem taxon_profile -c 96 MBS1384_refinem_genes <stats_output_dir>/scaffold_stats.tsv <reference_db> <reference_taxonomy> <taxon_profile_output_dir> (make sure databases are in the same working directory, weird)
refinem taxon_filter -c 40 <taxon_profile_dir> taxon_filter.tsv
refinem filter_bins --genome_ext .fa *product1 *temp3/taxon_filter.tsv MBS1384_refinem_product2 (check the folder if both filtered and unfiltered bins, get filtered ones)


Bin_Reassembly
metawrap reassemble_bins --parallel -o MBS1383_binreassembled -1 *_1.fastq -2 *_2.fastq -t 96 -m 824 -c 50 -x 5 -b *product2 (bins from refinem stage 2) - note: too damn long


How to take bins that passes the QS score to a new folder?

1) column -t metawrap_50_5_bins.stats | awk '{print $1, $2, $3, $2-(5*$3)}' | awk '$2>50 && $3<10 && $4>50' | awk '{print $1}' > metawrap_50_5_bins/1376_QS50_stats
2) rename 's/.fa//' *
3) mkdir new_folder
4) cp $(<1376_QS50_stats) new_folder
5) Edit the files again with rename to put things at front and put back the .fna 

Clue
for f in *.txt; do 
    mv -- "$f" "${f%.txt}.text"
done


dRep

1) Install. conda. Only need to have checkM
2) dRep dereplicate Bins_refined_all_drep -g Bins_refined_all_v2/** -p 12 -pa 0.90 -sa 0.95 -nc 0.30 -cm larger -comp 50 -con 5

GTDB-DK
1) Install. conda. DL the database separately. 
2) Connect the database = export GTDBTK_DATA_PATH=/mnt/volume1/release202/
3) gtdbtk classify_wf --cpus 48 --genome_dir Bins_refined_all_v1_insinglefolder --out_dir Bins_refined_all_v1_insinglefolder_gtdbtk_classifywf


16S rRNA detection and analysis

https://github.com/christophertbrown/bioscripts

1) Install as per github
  - Install infernal and SSU-Align
  - Download Database from their github and upload it to your system
  - Create the environment export ssucmdb="databases/ssu-align-0p1.1.cm"
2) Command
  -  16SfromHMM.py -f <<*84a/*carvrk/scaffolds.fa>> -m -t 16 > <<MBS1384carvrk16S.fa>>
3) Analysis
  - Download the .fa file
  - Silva ACT website. Enable Sina Search and Classify. 
  - Enable SILVA taxonomic classification (LCA's)
4) Report
  - Open .csv in R studio; be sure all libraries and dependencies there
  - E.g. Keep only scaffold ID, % identity, length of bp, taxonomy
  
        silva1383 <- read.csv("a.csv", head = TRUE, sep = ";")
        silva1383 <- silva1383[c(3,9,15,21)]

Libraries (for phylogeny and other CRISPR stuff)

1) Download all "complete genomes", chromosomes, scaffolds, contigs from assembly under bacteria and archaea (For bacteria scaffolds and contigs, 2015-01-01 and above)
2) Download meta txid 256318, 410656, 410657



Plass

trim_galore --stringency 5 --length 120 --quality 20 --max_n 2 --trim-n --cores 8 â€“paired MBS1384R1_1.fastq.gz MBS1384R1_2.fastq.gz
zcat MBS1384R1_1_trimming_min120.fastq.gz MBS1384R1_2_trimming_min120.fastq.gz | seqkit sample -p 0.5 -o MBS1384R1_12_min120_max10GB.fastq.gz -j 64 (-p parameters should have a final dataset of 20GB)
plass assemble *84/*_1*trimmed.fq.gz *84/*_2*trimmed.fq.gz MBS1384_plass MBS1384_plass_tmp --threads 64 --min-length 40 (do pullseq if needed)
pullseq for 100 aa only and look for complete proteins only *start and the end*
seqkit split and seqkit fx2tsv 

hmmscan

1) change hmm to new version

for i in `ls *hmm | sed 's/.hmm//g'`; do hmmconvert ${i}.hmm > ${i}_new.hmm;done
cat *_new.hmm >> database.hmm
hmmpress database.hmm

2) hmmscan -o hmmscan_results_completeprotein --tblout hmmtabletrial --noali -E 0.00001 --cpu 64 Profiles/databaseCas12 MBS1384/MBS1384_plass/MBS1384_completeprotein.fas

3) Parse (best hits to a list of fasta names)
hmmscan --tblout output_file.pfam Pfam-A.hmm seq_file.fasta
awk '!x[$3]++' ouput_file.pfam > MYBESTHITS.pfam

4) Get fasta 

seqtk subseq test.fa test.txt 
blastp the resulting fasta

Clean-up for Cas12j results
Note: Cas12j_proteins_consolidated_summary.tsv has operons with Cas12k homology but classified as Cas12j.

1) grep 'Cas12j' Cas12j_proteins_consolidated_summary.tsv | awk '{print $1}' > Cas12j_proteins_consolidated_summary_showingonlyCas12j.tsv 
# get operon names with Cas12j in their line (no operons with Cas12k has Cas12j as well)

2) seqkit grep -nrf Cas12j_proteins_consolidated_summary_showingonlynameswithCas12j.txt Cas12j_proteins_consolidated_sorted.fasta -o newfile.fasta
# things still have to be removed. some contigs have both Cas12j and Cas12k

Cas12a Cas12j manipulation

1) For Cas12j
  a) Include the Banfield/Doudna paper
  b) To be done - cluster with the other Cas12 proteins to be sure
  
2) For Cas12a (still has to be changed)
  a) Cas12a tblastn do cd-hit 95% to reduce number of proteins  ------- should be look at those with genome. Then crisprcastyper, then reduce.
  b) filter the original cas12a tblastn table to include only the cdhit 95
  c) Get the original genomes. if there are none, don't include them in cctyper.
  d) do cctyper. get only 
  
MAFFT

mafft --auto --amino input > output
FastTree Cas12j_all_cdhit95_v1_mafft.fasta > Cas12j_all_cdhit95_v1_mafft_fasttree.fasta
